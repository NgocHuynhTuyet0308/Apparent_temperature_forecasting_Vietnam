{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12411199,"sourceType":"datasetVersion","datasetId":7827318}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data preparation and design of LSTM - based Timeseries models and variants\nThis notebook presents the process of preparing input data and constructing various LSTM-based model variants to capture temporal patterns in apparent temperature.\n\n### Main idea:\nThe model is built for a multivariate time series forecasting task.\n\nInput: Preprocessed apparent temperature data with 6 descriptive features, including day, month, year, air temperature, dew point, and relative humidity.\n\nOutput: 2 target features such as mean apparent temperature, max apparent temperature.\n","metadata":{}},{"cell_type":"code","source":"import typing\nfrom itertools import islice\nimport pandas as pd\nimport numpy as np\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nimport keras\nfrom keras import layers, ops\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nfrom math import radians, sin, cos, sqrt, atan2\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:41.617761Z","iopub.execute_input":"2025-07-11T08:30:41.619085Z","iopub.status.idle":"2025-07-11T08:30:41.625531Z","shell.execute_reply.started":"2025-07-11T08:30:41.619036Z","shell.execute_reply":"2025-07-11T08:30:41.624434Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df_CaMau = pd.read_csv('/kaggle/input/at-vietnam-data/Data_AT_FilteredDate/CaMau_FilteredDate.csv')\ndf_LangSon = pd.read_csv('/kaggle/input/at-vietnam-data/Data_AT_FilteredDate/LangSon_FilteredDate.csv')\ndf_LaoCai = pd.read_csv('/kaggle/input/at-vietnam-data/Data_AT_FilteredDate/LaoCai_FilteredDate.csv')\ndf_NoiBai = pd.read_csv('/kaggle/input/at-vietnam-data/Data_AT_FilteredDate/NoiBai_FilteredDate.csv')\ndf_PhuBai = pd.read_csv('/kaggle/input/at-vietnam-data/Data_AT_FilteredDate/PhuBai_FilteredDate.csv')\ndf_QuyNhon = pd.read_csv('/kaggle/input/at-vietnam-data/Data_AT_FilteredDate/QuyNhon_FilteredDate.csv')\ndf_TPHCM = pd.read_csv('/kaggle/input/at-vietnam-data/Data_AT_FilteredDate/TPHCM_FilteredDate.csv')\ndf_Vinh = pd.read_csv('/kaggle/input/at-vietnam-data/Data_AT_FilteredDate/Vinh_FilteredDate.csv')\n\nstation_dfs = {\n    'Noi_Bai': df_NoiBai,\n    'Lang_Son': df_LangSon,\n    'Lao_Cai': df_LaoCai,\n\n    'Vinh': df_Vinh,\n    'Phu_Bai': df_PhuBai,\n    'Quy_Nhon': df_QuyNhon,\n\n    'TPHCM': df_TPHCM,\n    'Ca_Mau': df_CaMau\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:41.626883Z","iopub.execute_input":"2025-07-11T08:30:41.627292Z","iopub.status.idle":"2025-07-11T08:30:41.823556Z","shell.execute_reply.started":"2025-07-11T08:30:41.627256Z","shell.execute_reply":"2025-07-11T08:30:41.822321Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Data preparation\n\nData preparation steps:\n1. **Normalize data to range [0, 1] with MinMaxScaler**: Function scale_data_per_station\n   \n2. **Split dataset into X and y set**: The dataset is split into input features (X) and target variables (y), with X including descriptive features and y representing the target apparent temperature values to be predicted. Function: split_X_y\n\n   \n3. **Divide X and y into three sets (training, validation, and testing)**: Based on the 80 - 10 - 10 ratio, 80% of the data is used for training, 10% for validation, and 10% for testing. Function: split_train_val_test_set\n\n4. **Generate sliding windows for time series modeling**: Function create_tf_dataset\n","metadata":{}},{"cell_type":"code","source":"# Normalize data\ndef scale_data_per_station(station_data_dict, features, feature_range=(0, 1),\n                           handle_outliers=True, scaling_method='minmax'):\n    scaled_data_dict = {}\n    scaler_dict = {}\n    \n    for station_name, df in station_data_dict.items():\n        df_scaled = df.copy()\n        feature_scalers = {}\n        \n        for feature in features:\n            feature_data = df_scaled[feature].copy()\n            \n            if handle_outliers:\n                Q1 = feature_data.quantile(0.25)  \n                Q3 = feature_data.quantile(0.75)  \n                IQR = Q3 - Q1\n                lower_bound = Q1 - 1.5 * IQR\n                upper_bound = Q3 + 1.5 * IQR\n                \n                feature_data = feature_data.clip(lower_bound, upper_bound)\n            \n            feature_array = feature_data.values.reshape(-1, 1)\n            \n            # Chọn scaler phù hợp\n            if scaling_method == 'robust':\n                scaler = RobustScaler()\n                scaled_values = scaler.fit_transform(feature_array)\n            else:  # 'minmax'\n                scaler = MinMaxScaler(feature_range=feature_range)\n                scaled_values = scaler.fit_transform(feature_array)\n            \n            # Cập nhật DataFrame\n            df_scaled[feature] = scaled_values\n            feature_scalers[feature] = scaler\n            \n        scaled_data_dict[station_name] = df_scaled\n        scaler_dict[station_name] = feature_scalers\n        \n    return scaled_data_dict, scaler_dict\n\n\nfeatures_to_scale = ['YEAR', 'MONTH', 'DAY', 'DEW_2', 'TMP_2', 'RH', 'AT mean', 'AT max']\n\nscaled_data, scalers = scale_data_per_station(\n    station_dfs, \n    features_to_scale,\n    handle_outliers=False,\n    scaling_method='minmax' \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:41.824948Z","iopub.execute_input":"2025-07-11T08:30:41.825404Z","iopub.status.idle":"2025-07-11T08:30:41.883317Z","shell.execute_reply.started":"2025-07-11T08:30:41.825371Z","shell.execute_reply":"2025-07-11T08:30:41.882442Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Tách đặc trưng mô tả (X) và mục tiêu (y)\ndef split_X_y(df, X_features, y_features):\n    X_groups = df[X_features]\n    y_groups = df[y_features]\n    return np.array(X_groups), np.array(y_groups)\n    \nX_features = ['YEAR', 'MONTH', 'DAY', 'DEW_2', 'TMP_2', 'RH']\ny_features = ['AT mean', 'AT max']\nX, y = split_X_y(scaled_data['Ca_Mau'], X_features, y_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:41.885300Z","iopub.execute_input":"2025-07-11T08:30:41.886005Z","iopub.status.idle":"2025-07-11T08:30:41.900602Z","shell.execute_reply.started":"2025-07-11T08:30:41.885973Z","shell.execute_reply":"2025-07-11T08:30:41.899580Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Tách tập dữ liệu theo tỷ lệ 80 - 10 - 10% \ndef split_train_val_test_set(X, y, train_size=0.8, val_size=0.1):\n    num_timestamp = len(X)\n    num_train, num_val = (\n        int(num_timestamp * train_size),\n        int(num_timestamp * val_size)\n    )\n\n    X_train = X[:num_train]\n    y_train = y[:num_train]\n    X_val = X[num_train: (num_train + num_val)]\n    y_val = y[num_train: (num_train + num_val)]\n    X_test = X[(num_train + num_val):]\n    y_test = y[(num_train + num_val):]\n\n    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n\n(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_train_val_test_set(X, y)\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"y_val shape:\", y_val.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:41.901720Z","iopub.execute_input":"2025-07-11T08:30:41.902152Z","iopub.status.idle":"2025-07-11T08:30:41.923461Z","shell.execute_reply.started":"2025-07-11T08:30:41.902125Z","shell.execute_reply":"2025-07-11T08:30:41.922557Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (9408, 6)\ny_train shape: (9408, 2)\nX_val shape: (1176, 6)\ny_val shape: (1176, 2)\nX_test shape: (1177, 6)\ny_test shape: (1177, 2)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Tạo cửa sổ dữ liệu\n# Trạm miền Bắc + Trung: batch_size = 64\n# Trạm miền Nam: batch_size = 8\n\nbatch_size = 8\ninput_sequence_length = 31\nforecast_horizon = 1\nmulti_horizon = False\n\ndef create_tf_dataset(\n        X_full: np.ndarray,\n        y_full: np.ndarray,\n        input_sequence_length: int = 30,\n        forecast_horizon: int = 1,\n        batch_size: int = 64,\n        shuffle: bool = True,\n        multi_horizon: bool = False,\n):\n    inputs = keras.utils.timeseries_dataset_from_array(\n        X_full[:-forecast_horizon], \n        None,\n        sequence_length=input_sequence_length,\n        shuffle=False,\n        batch_size=batch_size\n    )\n\n    target_offset = (\n        input_sequence_length\n        if multi_horizon\n        else input_sequence_length + forecast_horizon - 1\n    )\n\n    target_seq_length = forecast_horizon if multi_horizon else 1\n\n    targets = keras.utils.timeseries_dataset_from_array(\n        y_full[target_offset:],\n        None,\n        sequence_length=target_seq_length,\n        shuffle=False,\n        batch_size=batch_size\n    )\n\n    dataset = tf.data.Dataset.zip((inputs, targets))\n\n    if shuffle:\n        dataset = dataset.shuffle(100)\n\n    return dataset.prefetch(16).cache()\n\n\ntrain_dataset = create_tf_dataset(\n    X_train, \n    y_train,\n    input_sequence_length=input_sequence_length,\n    forecast_horizon=forecast_horizon,\n    batch_size=batch_size,\n    shuffle=True,\n    multi_horizon=multi_horizon\n)\n\nval_dataset = create_tf_dataset(\n    X_val, \n    y_val,\n    input_sequence_length=input_sequence_length,\n    forecast_horizon=forecast_horizon,\n    batch_size=batch_size,\n    shuffle=True,\n    multi_horizon=multi_horizon\n)\n\ntest_dataset = create_tf_dataset(\n    X_test, \n    y_test,\n    input_sequence_length=input_sequence_length,\n    forecast_horizon=forecast_horizon,\n    batch_size=len(X_test),  # Sử dụng toàn bộ test set\n    shuffle=False,\n    multi_horizon=multi_horizon\n)\n\nprint(train_dataset)\nprint(val_dataset)\nprint(test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:41.924614Z","iopub.execute_input":"2025-07-11T08:30:41.924979Z","iopub.status.idle":"2025-07-11T08:30:42.367921Z","shell.execute_reply.started":"2025-07-11T08:30:41.924948Z","shell.execute_reply":"2025-07-11T08:30:42.366524Z"}},"outputs":[{"name":"stderr","text":"2025-07-11 08:30:41.941137: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"<CacheDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>\n<CacheDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>\n<CacheDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Building LSTM and BiLSTM models","metadata":{}},{"cell_type":"code","source":"class LSTM(layers.Layer):\n    def __init__(\n        self,\n        lstm_units: int,\n        input_seq_len: int,\n        in_feat: int,\n        forecast_horizon: int,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n\n        self.lstm1 = layers.LSTM(\n            lstm_units, \n            activation=\"tanh\",\n            return_sequences=True,\n        )\n\n        self.lstm2 = layers.LSTM(\n            lstm_units // 2,\n            activation=\"tanh\", \n            return_sequences=True\n        )\n\n\n        self.lstm = layers.LSTM(\n            lstm_units // 4,\n            activation=\"tanh\", \n            return_sequences=False\n        )\n\n        self.dense_32 = layers.Dense(32)\n        self.dense = layers.Dense(forecast_horizon * 2)\n\n        self.dropout_02 = layers.Dropout(0.02)\n        self.dropout_01 = layers.Dropout(0.1)\n        \n        self.input_seq_len = input_seq_len\n        self.in_feat = in_feat\n        self.forecast_horizon = forecast_horizon\n\n        self.norm = layers.LayerNormalization()\n\n    def call(self, inputs):\n        print(\"Input: \", inputs.shape)\n        \n        lstm1 = self.lstm1(inputs)\n        lstm1 = self.dropout_02(lstm1)\n        print(\"LSTM 1: \", lstm1.shape)\n\n        lstm2 = self.lstm2(lstm1)\n        lstm2 = self.dropout_02(lstm2)\n        print(\"LSTM 2: \", lstm2.shape)\n        \n        lstm_out = self.lstm(lstm2)\n        lstm_out = self.dropout_02(lstm_out)\n        print(\"LSTM out: \", lstm_out.shape)\n\n        dense1 = self.dense_32(lstm_out)\n        dense1 = self.dropout_02(dense1)\n        print(\"Dense 32 shape: \", dense1.shape)\n        \n        dense_output = self.dense(dense1)\n        print(\"Dense output shape: \", dense_output.shape)\n        \n        batch_size = tf.shape(dense_output)[0]  \n        output = tf.reshape(dense_output, (batch_size, self.forecast_horizon, 2))\n        \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:42.369183Z","iopub.execute_input":"2025-07-11T08:30:42.369584Z","iopub.status.idle":"2025-07-11T08:30:42.385280Z","shell.execute_reply.started":"2025-07-11T08:30:42.369552Z","shell.execute_reply":"2025-07-11T08:30:42.384083Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class BiLSTM(layers.Layer):\n    def __init__(\n        self, \n        lstm_units: int,\n        input_seq_len: int,\n        in_feat: int,\n        forecast_horizon: int,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        \n        initializer = tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08)\n\n        self.bilstm1 = layers.Bidirectional(\n            layers.LSTM(\n                lstm_units,\n                activation=\"tanh\",\n                return_sequences=True,\n            ),\n            merge_mode='ave'\n        )\n\n        self.bilstm2 = layers.Bidirectional(\n            layers.LSTM(\n                lstm_units // 2,\n                activation=\"tanh\",\n                return_sequences=True\n            ),\n            merge_mode='ave'\n        )\n\n        self.bilstm3 = layers.Bidirectional(\n            layers.LSTM(\n                lstm_units // 4,\n                activation=\"tanh\",\n                return_sequences=False\n            ),\n            merge_mode='ave'\n        )\n\n        self.dense_32 = layers.Dense(32)\n        self.dense = layers.Dense(2)\n\n        self.dropout_02 = layers.Dropout(0.2)\n        self.dropout_01 = layers.Dropout(0.02) #0.01       \n        \n        self.input_seq_len = input_seq_len\n        self.in_feat = in_feat\n        self.forecast_horizon = forecast_horizon\n\n\n    def call(self, inputs):\n        bilstm1 = self.bilstm1(inputs)\n        bilstm1 = self.dropout_02(bilstm1)\n        \n        bilstm2 = self.bilstm2(bilstm1)\n        bilstm2 = self.dropout_02(bilstm2)\n        \n        bilstm_out = self.bilstm3(bilstm2)\n        bilstm_out = self.dropout_02(bilstm_out)\n\n        dense1 = self.dense_32(bilstm_out)\n        dense1 = self.dropout_01(dense1)\n        \n        dense_output = self.dense(dense1)\n        # dense_output = self.dropout_01(dense_output)\n    \n        batch_size = tf.shape(dense_output)[0]  \n        output = tf.reshape(dense_output, (batch_size, self.forecast_horizon, 2))\n        \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:42.386611Z","iopub.execute_input":"2025-07-11T08:30:42.387129Z","iopub.status.idle":"2025-07-11T08:30:42.413175Z","shell.execute_reply.started":"2025-07-11T08:30:42.387053Z","shell.execute_reply":"2025-07-11T08:30:42.411811Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"in_feat = 6\nepochs = 1000\nlstm_units = 256\nforecast_horizon = 1\n\n\nlstm_predictor = LSTM(\n    lstm_units=lstm_units,\n    input_seq_len=input_sequence_length,\n    in_feat = in_feat,\n    forecast_horizon = forecast_horizon\n)\n\nbilstm_predictor = BiLSTM(\n    lstm_units=lstm_units,\n    input_seq_len=input_sequence_length,\n    in_feat = in_feat,\n    forecast_horizon = forecast_horizon\n)\n\ninputs = layers.Input(shape=(input_sequence_length, in_feat))\noutputs = bilstm_predictor(inputs)\n\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=0.0001,\n    clipnorm=1.0,\n)\nmodel = keras.models.Model(inputs, outputs)\nmodel.compile(\n    loss='mean_squared_error',\n    optimizer=optimizer,\n    metrics=[\"mean_squared_error\"]\n)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:42.414819Z","iopub.execute_input":"2025-07-11T08:30:42.415276Z","iopub.status.idle":"2025-07-11T08:30:44.094734Z","shell.execute_reply.started":"2025-07-11T08:30:42.415244Z","shell.execute_reply":"2025-07-11T08:30:44.094000Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m6\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bi_lstm (\u001b[38;5;33mBiLSTM\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)           │     \u001b[38;5;34m1,033,826\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bi_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BiLSTM</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,033,826</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,033,826\u001b[0m (3.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,033,826</span> (3.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,033,826\u001b[0m (3.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,033,826</span> (3.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import time\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Thiết lập Early Stopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss', \n    patience=5,  \n    restore_best_weights=True\n)\n\nstart_time = time.time()\n\n# Huấn luyện mô hình\nhistory = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=1000,\n    callbacks=[early_stopping],\n)\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint(training_time)\n\n# Chuyển đổi sang định dạng giờ:phút:giây\nhours, remainder = divmod(training_time, 3600)\nminutes, seconds = divmod(remainder, 60)\n\nprint(f'Thời gian huấn luyện: {int(hours)} giờ {int(minutes)} phút {seconds:.2f} giây')\nprint(f'Tổng số epoch đã huấn luyện: {len(history.history[\"loss\"])}')\nprint(f'Giá trị loss cuối cùng: {history.history[\"loss\"][-1]:.4f}')\nprint(f'Giá trị validation loss cuối cùng: {history.history[\"val_loss\"][-1]:.4f}')\n\n# Tính thời gian trung bình cho mỗi epoch\navg_time_per_epoch = training_time / len(history.history[\"loss\"])\nprint(f'Thời gian trung bình cho mỗi epoch: {avg_time_per_epoch:.2f} giây')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:30:44.097642Z","iopub.execute_input":"2025-07-11T08:30:44.097917Z","execution_failed":"2025-07-11T08:31:17.982Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/1000\n\u001b[1m 162/1173\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 114ms/step - loss: 0.0470 - mean_squared_error: 0.0470","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(history.history['loss'], label='Loss (huấn luyện)')\nplt.plot(history.history['val_loss'], label='Loss (xác thực)')\nplt.xlabel('Epoch', fontsize=12, fontfamily='Times New Roman')\nplt.ylabel('Giá trị loss', fontsize=12, fontfamily='Times New Roman')\nplt.title('GIÁ TRỊ LOSS TRONG QUÁ TRÌNH HUẤN LUYỆN VÀ XÁC THỰC', fontsize=14, fontfamily='Times New Roman', fontweight='bold')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-11T08:31:17.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test, y = next(test_dataset.as_numpy_iterator())\ny_pred = model.predict(x_test)\n\nprint(\"x test shape: \", x_test.shape)\nprint(\"y shape: \", y.shape)\nprint(\"y pred shape: \", y_pred.shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-11T08:31:17.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train, y_train = next(train_dataset.as_numpy_iterator())\nprint(\"x train shape: \", x_train.shape)\nprint(\"y train shape: \", y_train.shape)\n\nx_val, y_val = next(val_dataset.as_numpy_iterator())\nprint(\"x val shape: \", x_val.shape)\nprint(\"y val shape: \", y_val.shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-11T08:31:17.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Đếm số batch trong train_dataset\ntrain_batches = 0\nfor _ in train_dataset:\n    train_batches += 1\nprint(f\"Số lượng batch trong train_dataset: {train_batches}\")\n\n# Đếm số batch trong val_dataset\nval_batches = 0\nfor _ in val_dataset:\n    val_batches += 1\nprint(f\"Số lượng batch trong val_dataset: {val_batches}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-11T08:31:17.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Đếm chính xác số mẫu trong train_dataset\ntrain_samples = 0\nfor x_batch, _ in train_dataset:\n    train_samples += x_batch.shape[0]\nprint(f\"Số lượng mẫu thực tế trong train_dataset: {train_samples}\")\n\n# Đếm chính xác số mẫu trong val_dataset\nval_samples = 0\nfor x_batch, _ in val_dataset:\n    val_samples += x_batch.shape[0]\nprint(f\"Số lượng mẫu thực tế trong val_dataset: {val_samples}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-11T08:31:17.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_all(y_true, y_pred):\n    y_test_flat = y_true.reshape(-1, y_true.shape[-1]) \n    y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n    \n    r2 = r2_score(y_test_flat, y_pred_flat)\n    mse = mean_squared_error(y_test_flat, y_pred_flat)\n    rmse = np.sqrt(mean_squared_error(y_test_flat, y_pred_flat))\n    mae = mean_absolute_error(y_test_flat, y_pred_flat)\n    \n    print(f\"R²: {r2 * 100:.4f}%\")\n    print(f\"MSE: {mse:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE: {mae:.4f}\")\n\nevaluate_all(y, y_pred) ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-11T08:31:17.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_all_by_target_features(y_true, y_pred):\n    y_true_flat = y_true.reshape(-1, y_true.shape[-1]) \n    y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n\n    for i in range(y_true_flat.shape[1]):\n        y_true_feature = y_true_flat[:, i]\n        y_pred_feature = y_pred_flat[:, i]\n        \n        r2 = r2_score(y_true_feature, y_pred_feature)\n        mse = mean_squared_error(y_true_feature, y_pred_feature)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(y_true_feature, y_pred_feature)\n        \n        print(f\"Feature {i + 1}:\")\n        print(f\"  R²: {r2 * 100:.4f}%\")\n        print(f\"  MSE: {mse:.4f}\")\n        print(f\"  RMSE: {rmse:.4f}\")\n        print(f\"  MAE: {mae:.4f}\")\n        print(\"-\" * 30)\n\nevaluate_all_by_target_features(y, y_pred)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-11T08:31:17.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_features=['AT mean', 'AT max']\nstation = 'Ca Mau'\ny_true_flat = y.reshape(-1, y.shape[-1])\ny_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n    \n# Khởi tạo mảng để lưu dữ liệu đã inverse transform\ny_true_original = np.copy(y_true_flat)\ny_pred_original = np.copy(y_pred_flat)\n    \n# Scale ngược dữ liệu nếu scalers được cung cấp\nif scalers is not None and station in scalers:\n    station_scalers = scalers[station]\n        \n    for i, feature in enumerate(target_features):\n        if feature in station_scalers:\n            feature_scaler = station_scalers[feature]\n                \n            # Inverse transform từng cột riêng biệt\n            y_true_feature = y_true_flat[:, i].reshape(-1, 1)\n            y_pred_feature = y_pred_flat[:, i].reshape(-1, 1)\n                \n            y_true_original[:, i:i+1] = feature_scaler.inverse_transform(y_true_feature)\n            y_pred_original[:, i:i+1] = feature_scaler.inverse_transform(y_pred_feature)\n\nfor i, feature in enumerate(target_features):\n    y_true_feature = y_true_original[:, i]\n    y_pred_feature = y_pred_original[:, i]\n        \n    r2 = r2_score(y_true_feature, y_pred_feature)\n    mse = mean_squared_error(y_true_feature, y_pred_feature)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true_feature, y_pred_feature)\n        \n    print(f\"{feature}:\")\n    print(f\"  Hệ số xác định (R²): {r2 * 100:.3f}%\")\n    print(f\"  Sai số bình phương trung bình (MSE): {mse:.3f}\")\n    print(f\"  Căn sai số bình phương trung bình (RMSE): {rmse:.3f}\")\n    print(f\"  Sai số tuyệt đối trung bình (MAE): {mae:.3f}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-11T08:31:17.983Z"}},"outputs":[],"execution_count":null}]}