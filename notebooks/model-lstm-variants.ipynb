{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and design of LSTM - based Timeseries models and variants\n",
    "This notebook presents the process of preparing input data and constructing various LSTM-based model variants to capture temporal patterns in apparent temperature.\n",
    "\n",
    "### Main idea:\n",
    "The model is built for a multivariate time series forecasting task.\n",
    "\n",
    "Input: Preprocessed apparent temperature data with 6 descriptive features, including day, month, year, air temperature, dew point, and relative humidity.\n",
    "\n",
    "Output: 2 target features such as mean apparent temperature, max apparent temperature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:41.619085Z",
     "iopub.status.busy": "2025-07-11T08:30:41.617761Z",
     "iopub.status.idle": "2025-07-11T08:30:41.625531Z",
     "shell.execute_reply": "2025-07-11T08:30:41.624434Z",
     "shell.execute_reply.started": "2025-07-11T08:30:41.619036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import keras\n",
    "from keras import layers, ops\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:41.627292Z",
     "iopub.status.busy": "2025-07-11T08:30:41.626883Z",
     "iopub.status.idle": "2025-07-11T08:30:41.823556Z",
     "shell.execute_reply": "2025-07-11T08:30:41.822321Z",
     "shell.execute_reply.started": "2025-07-11T08:30:41.627256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_CaMau = pd.read_csv('./Data_AT_FilteredDate/CaMau_FilteredDate.csv')\n",
    "df_LangSon = pd.read_csv('./Data_AT_FilteredDate/LangSon_FilteredDate.csv')\n",
    "df_LaoCai = pd.read_csv('./Data_AT_FilteredDate/LaoCai_FilteredDate.csv')\n",
    "df_NoiBai = pd.read_csv('./Data_AT_FilteredDate/NoiBai_FilteredDate.csv')\n",
    "df_PhuBai = pd.read_csv('./Data_AT_FilteredDate/PhuBai_FilteredDate.csv')\n",
    "df_QuyNhon = pd.read_csv('./Data_AT_FilteredDate/QuyNhon_FilteredDate.csv')\n",
    "df_TPHCM = pd.read_csv('./Data_AT_FilteredDate/TPHCM_FilteredDate.csv')\n",
    "df_Vinh = pd.read_csv('./Data_AT_FilteredDate/Vinh_FilteredDate.csv')\n",
    "\n",
    "station_dfs = {\n",
    "    'Noi_Bai': df_NoiBai,\n",
    "    'Lang_Son': df_LangSon,\n",
    "    'Lao_Cai': df_LaoCai,\n",
    "\n",
    "    'Vinh': df_Vinh,\n",
    "    'Phu_Bai': df_PhuBai,\n",
    "    'Quy_Nhon': df_QuyNhon,\n",
    "\n",
    "    'TPHCM': df_TPHCM,\n",
    "    'Ca_Mau': df_CaMau\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Data preparation steps:\n",
    "1. **Normalize data to range [0, 1] with MinMaxScaler**: Function scale_data_per_station\n",
    "   \n",
    "2. **Split dataset into X and y set**: The dataset is split into input features (X) and target variables (y), with X including descriptive features and y representing the target apparent temperature values to be predicted. Function: split_X_y\n",
    "\n",
    "   \n",
    "3. **Divide X and y into three sets (training, validation, and testing)**: Based on the 80 - 10 - 10 ratio, 80% of the data is used for training, 10% for validation, and 10% for testing. Function: split_train_val_test_set\n",
    "\n",
    "4. **Generate sliding windows for time series modeling**: Function create_tf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:41.825404Z",
     "iopub.status.busy": "2025-07-11T08:30:41.824948Z",
     "iopub.status.idle": "2025-07-11T08:30:41.883317Z",
     "shell.execute_reply": "2025-07-11T08:30:41.882442Z",
     "shell.execute_reply.started": "2025-07-11T08:30:41.825371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def scale_data_per_station(station_data_dict, features, feature_range=(0, 1),\n",
    "                           handle_outliers=True, scaling_method='minmax'):\n",
    "    scaled_data_dict = {}\n",
    "    scaler_dict = {}\n",
    "    \n",
    "    for station_name, df in station_data_dict.items():\n",
    "        df_scaled = df.copy()\n",
    "        feature_scalers = {}\n",
    "        \n",
    "        for feature in features:\n",
    "            feature_data = df_scaled[feature].copy()\n",
    "            \n",
    "            if handle_outliers:\n",
    "                Q1 = feature_data.quantile(0.25)  \n",
    "                Q3 = feature_data.quantile(0.75)  \n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                feature_data = feature_data.clip(lower_bound, upper_bound)\n",
    "            \n",
    "            feature_array = feature_data.values.reshape(-1, 1)\n",
    "            \n",
    "            # Chọn scaler phù hợp\n",
    "            if scaling_method == 'robust':\n",
    "                scaler = RobustScaler()\n",
    "                scaled_values = scaler.fit_transform(feature_array)\n",
    "            else:  # 'minmax'\n",
    "                scaler = MinMaxScaler(feature_range=feature_range)\n",
    "                scaled_values = scaler.fit_transform(feature_array)\n",
    "            \n",
    "            # Cập nhật DataFrame\n",
    "            df_scaled[feature] = scaled_values\n",
    "            feature_scalers[feature] = scaler\n",
    "            \n",
    "        scaled_data_dict[station_name] = df_scaled\n",
    "        scaler_dict[station_name] = feature_scalers\n",
    "        \n",
    "    return scaled_data_dict, scaler_dict\n",
    "\n",
    "\n",
    "features_to_scale = ['YEAR', 'MONTH', 'DAY', 'DEW_2', 'TMP_2', 'RH', 'AT mean', 'AT max']\n",
    "\n",
    "scaled_data, scalers = scale_data_per_station(\n",
    "    station_dfs, \n",
    "    features_to_scale,\n",
    "    handle_outliers=False,\n",
    "    scaling_method='minmax' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:41.886005Z",
     "iopub.status.busy": "2025-07-11T08:30:41.885300Z",
     "iopub.status.idle": "2025-07-11T08:30:41.900602Z",
     "shell.execute_reply": "2025-07-11T08:30:41.899580Z",
     "shell.execute_reply.started": "2025-07-11T08:30:41.885973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tách đặc trưng mô tả (X) và mục tiêu (y)\n",
    "def split_X_y(df, X_features, y_features):\n",
    "    X_groups = df[X_features]\n",
    "    y_groups = df[y_features]\n",
    "    return np.array(X_groups), np.array(y_groups)\n",
    "    \n",
    "X_features = ['YEAR', 'MONTH', 'DAY', 'DEW_2', 'TMP_2', 'RH']\n",
    "y_features = ['AT mean', 'AT max']\n",
    "X, y = split_X_y(scaled_data['Ca_Mau'], X_features, y_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:41.902152Z",
     "iopub.status.busy": "2025-07-11T08:30:41.901720Z",
     "iopub.status.idle": "2025-07-11T08:30:41.923461Z",
     "shell.execute_reply": "2025-07-11T08:30:41.922557Z",
     "shell.execute_reply.started": "2025-07-11T08:30:41.902125Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9408, 6)\n",
      "y_train shape: (9408, 2)\n",
      "X_val shape: (1176, 6)\n",
      "y_val shape: (1176, 2)\n",
      "X_test shape: (1177, 6)\n",
      "y_test shape: (1177, 2)\n"
     ]
    }
   ],
   "source": [
    "# Tách tập dữ liệu theo tỷ lệ 80 - 10 - 10% \n",
    "def split_train_val_test_set(X, y, train_size=0.8, val_size=0.1):\n",
    "    num_timestamp = len(X)\n",
    "    num_train, num_val = (\n",
    "        int(num_timestamp * train_size),\n",
    "        int(num_timestamp * val_size)\n",
    "    )\n",
    "\n",
    "    X_train = X[:num_train]\n",
    "    y_train = y[:num_train]\n",
    "    X_val = X[num_train: (num_train + num_val)]\n",
    "    y_val = y[num_train: (num_train + num_val)]\n",
    "    X_test = X[(num_train + num_val):]\n",
    "    y_test = y[(num_train + num_val):]\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_train_val_test_set(X, y)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:41.924979Z",
     "iopub.status.busy": "2025-07-11T08:30:41.924614Z",
     "iopub.status.idle": "2025-07-11T08:30:42.367921Z",
     "shell.execute_reply": "2025-07-11T08:30:42.366524Z",
     "shell.execute_reply.started": "2025-07-11T08:30:41.924948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CacheDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>\n",
      "<CacheDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>\n",
      "<CacheDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Tạo cửa sổ dữ liệu\n",
    "# Trạm miền Bắc + Trung: batch_size = 64\n",
    "# Trạm miền Nam: batch_size = 8\n",
    "\n",
    "batch_size = 8\n",
    "input_sequence_length = 31\n",
    "forecast_horizon = 1\n",
    "multi_horizon = False\n",
    "\n",
    "def create_tf_dataset(\n",
    "        X_full: np.ndarray,\n",
    "        y_full: np.ndarray,\n",
    "        input_sequence_length: int = 30,\n",
    "        forecast_horizon: int = 1,\n",
    "        batch_size: int = 64,\n",
    "        shuffle: bool = True,\n",
    "        multi_horizon: bool = False,\n",
    "):\n",
    "    inputs = keras.utils.timeseries_dataset_from_array(\n",
    "        X_full[:-forecast_horizon], \n",
    "        None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    target_offset = (\n",
    "        input_sequence_length\n",
    "        if multi_horizon\n",
    "        else input_sequence_length + forecast_horizon - 1\n",
    "    )\n",
    "\n",
    "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "\n",
    "    targets = keras.utils.timeseries_dataset_from_array(\n",
    "        y_full[target_offset:],\n",
    "        None,\n",
    "        sequence_length=target_seq_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "    return dataset.prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_dataset = create_tf_dataset(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    input_sequence_length=input_sequence_length,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    multi_horizon=multi_horizon\n",
    ")\n",
    "\n",
    "val_dataset = create_tf_dataset(\n",
    "    X_val, \n",
    "    y_val,\n",
    "    input_sequence_length=input_sequence_length,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    multi_horizon=multi_horizon\n",
    ")\n",
    "\n",
    "test_dataset = create_tf_dataset(\n",
    "    X_test, \n",
    "    y_test,\n",
    "    input_sequence_length=input_sequence_length,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    batch_size=len(X_test),  # Sử dụng toàn bộ test set\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon\n",
    ")\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LSTM and BiLSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:42.369584Z",
     "iopub.status.busy": "2025-07-11T08:30:42.369183Z",
     "iopub.status.idle": "2025-07-11T08:30:42.385280Z",
     "shell.execute_reply": "2025-07-11T08:30:42.384083Z",
     "shell.execute_reply.started": "2025-07-11T08:30:42.369552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LSTM(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lstm_units: int,\n",
    "        input_seq_len: int,\n",
    "        in_feat: int,\n",
    "        forecast_horizon: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "        self.lstm1 = layers.LSTM(\n",
    "            lstm_units, \n",
    "            activation=\"tanh\",\n",
    "            return_sequences=True,\n",
    "        )\n",
    "\n",
    "        self.lstm2 = layers.LSTM(\n",
    "            lstm_units // 2,\n",
    "            activation=\"tanh\", \n",
    "            return_sequences=True\n",
    "        )\n",
    "\n",
    "\n",
    "        self.lstm = layers.LSTM(\n",
    "            lstm_units // 4,\n",
    "            activation=\"tanh\", \n",
    "            return_sequences=False\n",
    "        )\n",
    "\n",
    "        self.dense_32 = layers.Dense(32)\n",
    "        self.dense = layers.Dense(forecast_horizon * 2)\n",
    "\n",
    "        self.dropout_02 = layers.Dropout(0.02)\n",
    "        self.dropout_01 = layers.Dropout(0.1)\n",
    "        \n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.in_feat = in_feat\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        self.norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"Input: \", inputs.shape)\n",
    "        \n",
    "        lstm1 = self.lstm1(inputs)\n",
    "        lstm1 = self.dropout_02(lstm1)\n",
    "        print(\"LSTM 1: \", lstm1.shape)\n",
    "\n",
    "        lstm2 = self.lstm2(lstm1)\n",
    "        lstm2 = self.dropout_02(lstm2)\n",
    "        print(\"LSTM 2: \", lstm2.shape)\n",
    "        \n",
    "        lstm_out = self.lstm(lstm2)\n",
    "        lstm_out = self.dropout_02(lstm_out)\n",
    "        print(\"LSTM out: \", lstm_out.shape)\n",
    "\n",
    "        dense1 = self.dense_32(lstm_out)\n",
    "        dense1 = self.dropout_02(dense1)\n",
    "        print(\"Dense 32 shape: \", dense1.shape)\n",
    "        \n",
    "        dense_output = self.dense(dense1)\n",
    "        print(\"Dense output shape: \", dense_output.shape)\n",
    "        \n",
    "        batch_size = tf.shape(dense_output)[0]  \n",
    "        output = tf.reshape(dense_output, (batch_size, self.forecast_horizon, 2))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:42.387129Z",
     "iopub.status.busy": "2025-07-11T08:30:42.386611Z",
     "iopub.status.idle": "2025-07-11T08:30:42.413175Z",
     "shell.execute_reply": "2025-07-11T08:30:42.411811Z",
     "shell.execute_reply.started": "2025-07-11T08:30:42.387053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM(layers.Layer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        lstm_units: int,\n",
    "        input_seq_len: int,\n",
    "        in_feat: int,\n",
    "        forecast_horizon: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        initializer = tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08)\n",
    "\n",
    "        self.bilstm1 = layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                lstm_units,\n",
    "                activation=\"tanh\",\n",
    "                return_sequences=True,\n",
    "            ),\n",
    "            merge_mode='ave'\n",
    "        )\n",
    "\n",
    "        self.bilstm2 = layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                lstm_units // 2,\n",
    "                activation=\"tanh\",\n",
    "                return_sequences=True\n",
    "            ),\n",
    "            merge_mode='ave'\n",
    "        )\n",
    "\n",
    "        self.bilstm3 = layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                lstm_units // 4,\n",
    "                activation=\"tanh\",\n",
    "                return_sequences=False\n",
    "            ),\n",
    "            merge_mode='ave'\n",
    "        )\n",
    "\n",
    "        self.dense_32 = layers.Dense(32)\n",
    "        self.dense = layers.Dense(2)\n",
    "\n",
    "        self.dropout_02 = layers.Dropout(0.2)\n",
    "        self.dropout_01 = layers.Dropout(0.02) #0.01       \n",
    "        \n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.in_feat = in_feat\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        bilstm1 = self.bilstm1(inputs)\n",
    "        bilstm1 = self.dropout_02(bilstm1)\n",
    "        \n",
    "        bilstm2 = self.bilstm2(bilstm1)\n",
    "        bilstm2 = self.dropout_02(bilstm2)\n",
    "        \n",
    "        bilstm_out = self.bilstm3(bilstm2)\n",
    "        bilstm_out = self.dropout_02(bilstm_out)\n",
    "\n",
    "        dense1 = self.dense_32(bilstm_out)\n",
    "        dense1 = self.dropout_01(dense1)\n",
    "        \n",
    "        dense_output = self.dense(dense1)\n",
    "        # dense_output = self.dropout_01(dense_output)\n",
    "    \n",
    "        batch_size = tf.shape(dense_output)[0]  \n",
    "        output = tf.reshape(dense_output, (batch_size, self.forecast_horizon, 2))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:30:42.415276Z",
     "iopub.status.busy": "2025-07-11T08:30:42.414819Z",
     "iopub.status.idle": "2025-07-11T08:30:44.094734Z",
     "shell.execute_reply": "2025-07-11T08:30:44.094000Z",
     "shell.execute_reply.started": "2025-07-11T08:30:42.415244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bi_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BiLSTM</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,033,826</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m6\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bi_lstm (\u001b[38;5;33mBiLSTM\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)           │     \u001b[38;5;34m1,033,826\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,033,826</span> (3.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,033,826\u001b[0m (3.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,033,826</span> (3.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,033,826\u001b[0m (3.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = 6\n",
    "epochs = 1000\n",
    "lstm_units = 256\n",
    "forecast_horizon = 1\n",
    "\n",
    "\n",
    "lstm_predictor = LSTM(\n",
    "    lstm_units=lstm_units,\n",
    "    input_seq_len=input_sequence_length,\n",
    "    in_feat = in_feat,\n",
    "    forecast_horizon = forecast_horizon\n",
    ")\n",
    "\n",
    "bilstm_predictor = BiLSTM(\n",
    "    lstm_units=lstm_units,\n",
    "    input_seq_len=input_sequence_length,\n",
    "    in_feat = in_feat,\n",
    "    forecast_horizon = forecast_horizon\n",
    ")\n",
    "\n",
    "inputs = layers.Input(shape=(input_sequence_length, in_feat))\n",
    "outputs = bilstm_predictor(inputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0001,\n",
    "    clipnorm=1.0,\n",
    ")\n",
    "model = keras.models.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"mean_squared_error\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.982Z",
     "iopub.execute_input": "2025-07-11T08:30:44.097917Z",
     "iopub.status.busy": "2025-07-11T08:30:44.097642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 94ms/step - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "Epoch 2/1000\n",
      "\u001b[1m  85/1173\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 79ms/step - loss: 0.0114 - mean_squared_error: 0.0114"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m start_time = time.time()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Huấn luyện mô hình\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m end_time = time.time()\n\u001b[32m     22\u001b[39m training_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:378\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    376\u001b[39m callbacks.on_train_batch_begin(step)\n\u001b[32m    377\u001b[39m logs = \u001b[38;5;28mself\u001b[39m.train_function(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:172\u001b[39m, in \u001b[36mCallbackList.on_train_batch_end\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mself\u001b[39m._async_dispatch(\u001b[38;5;28mself\u001b[39m._on_train_batch_end, batch, logs)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_on_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:194\u001b[39m, in \u001b[36mCallbackList._on_train_batch_end\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m    192\u001b[39m logs = python_utils.pythonify_logs(logs)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks\\progbar_logger.py:58\u001b[39m, in \u001b[36mProgbarLogger.on_train_batch_end\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks\\progbar_logger.py:95\u001b[39m, in \u001b[36mProgbarLogger._update_progbar\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.seen = batch + \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# One-indexed.\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprogbar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\progbar.py:182\u001b[39m, in \u001b[36mProgbar.update\u001b[39m\u001b[34m(self, current, values, finalize)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m finalize:\n\u001b[32m    180\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[43mio_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m._prev_total_width = total_width\n\u001b[32m    184\u001b[39m message = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Huynh Thi Tuyet Ngoc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\io_utils.py:98\u001b[39m, in \u001b[36mprint_msg\u001b[39m\u001b[34m(message, line_break)\u001b[39m\n\u001b[32m     96\u001b[39m message = message + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m line_break \u001b[38;5;28;01melse\u001b[39;00m message\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# If the encoding differs from UTF-8, `sys.stdout.write` may fail.\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# To address this, replace special unicode characters in the\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# message, and then encode and decode using the target encoding.\u001b[39;00m\n\u001b[32m    103\u001b[39m     message = _replace_special_unicode_character(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3045\u001b[39m, in \u001b[36mInteractiveShell._tee.<locals>.write\u001b[39m\u001b[34m(data, *args, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(data, *args, **kwargs):\n\u001b[32m   3044\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write data to both the original destination and the capture dictionary.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m     result = \u001b[43moriginal_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   3047\u001b[39m         [\n\u001b[32m   3048\u001b[39m             \u001b[38;5;28mself\u001b[39m.display_pub.is_publishing,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3051\u001b[39m         ]\n\u001b[32m   3052\u001b[39m     ):\n\u001b[32m   3053\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\iostream.py:694\u001b[39m, in \u001b[36mOutStream.write\u001b[39m\u001b[34m(self, string)\u001b[39m\n\u001b[32m    692\u001b[39m     \u001b[38;5;28mself\u001b[39m.pub_thread.schedule(\u001b[38;5;28mself\u001b[39m._flush)\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_schedule_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\iostream.py:590\u001b[39m, in \u001b[36mOutStream._schedule_flush\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_schedule_in_thread\u001b[39m():\n\u001b[32m    588\u001b[39m     \u001b[38;5;28mself\u001b[39m._io_loop.call_later(\u001b[38;5;28mself\u001b[39m.flush_interval, \u001b[38;5;28mself\u001b[39m._flush)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpub_thread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_schedule_in_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\iostream.py:267\u001b[39m, in \u001b[36mIOPubThread.schedule\u001b[39m\u001b[34m(self, f)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28mself\u001b[39m._events.append(f)\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event_pipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    269\u001b[39m     f()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\zmq\\sugar\\socket.py:698\u001b[39m, in \u001b[36mSocket.send\u001b[39m\u001b[34m(self, data, flags, copy, track, routing_id, group)\u001b[39m\n\u001b[32m    691\u001b[39m         data = zmq.Frame(\n\u001b[32m    692\u001b[39m             data,\n\u001b[32m    693\u001b[39m             track=track,\n\u001b[32m    694\u001b[39m             copy=copy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    695\u001b[39m             copy_threshold=\u001b[38;5;28mself\u001b[39m.copy_threshold,\n\u001b[32m    696\u001b[39m         )\n\u001b[32m    697\u001b[39m     data.group = group\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\zmq\\backend\\cython\\_zmq.py:1137\u001b[39m, in \u001b[36mzmq.backend.cython._zmq.Socket.send\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\zmq\\backend\\cython\\_zmq.py:1185\u001b[39m, in \u001b[36mzmq.backend.cython._zmq.Socket.send\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\zmq\\backend\\cython\\_zmq.py:1453\u001b[39m, in \u001b[36mzmq.backend.cython._zmq._send_copy\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\zmq\\backend\\cython\\_zmq.py:176\u001b[39m, in \u001b[36mzmq.backend.cython._zmq._check_rc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Thiết lập Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5,  \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(training_time)\n",
    "\n",
    "# Chuyển đổi sang định dạng giờ:phút:giây\n",
    "hours, remainder = divmod(training_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(f'Thời gian huấn luyện: {int(hours)} giờ {int(minutes)} phút {seconds:.2f} giây')\n",
    "print(f'Tổng số epoch đã huấn luyện: {len(history.history[\"loss\"])}')\n",
    "print(f'Giá trị loss cuối cùng: {history.history[\"loss\"][-1]:.4f}')\n",
    "print(f'Giá trị validation loss cuối cùng: {history.history[\"val_loss\"][-1]:.4f}')\n",
    "\n",
    "# Tính thời gian trung bình cho mỗi epoch\n",
    "avg_time_per_epoch = training_time / len(history.history[\"loss\"])\n",
    "print(f'Thời gian trung bình cho mỗi epoch: {avg_time_per_epoch:.2f} giây')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Loss (huấn luyện)')\n",
    "plt.plot(history.history['val_loss'], label='Loss (xác thực)')\n",
    "plt.xlabel('Epoch', fontsize=12, fontfamily='Times New Roman')\n",
    "plt.ylabel('Giá trị loss', fontsize=12, fontfamily='Times New Roman')\n",
    "plt.title('GIÁ TRỊ LOSS TRONG QUÁ TRÌNH HUẤN LUYỆN VÀ XÁC THỰC', fontsize=14, fontfamily='Times New Roman', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_test, y = next(test_dataset.as_numpy_iterator())\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"x test shape: \", x_test.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "print(\"y pred shape: \", y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = next(train_dataset.as_numpy_iterator())\n",
    "print(\"x train shape: \", x_train.shape)\n",
    "print(\"y train shape: \", y_train.shape)\n",
    "\n",
    "x_val, y_val = next(val_dataset.as_numpy_iterator())\n",
    "print(\"x val shape: \", x_val.shape)\n",
    "print(\"y val shape: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đếm số batch trong train_dataset\n",
    "train_batches = 0\n",
    "for _ in train_dataset:\n",
    "    train_batches += 1\n",
    "print(f\"Số lượng batch trong train_dataset: {train_batches}\")\n",
    "\n",
    "# Đếm số batch trong val_dataset\n",
    "val_batches = 0\n",
    "for _ in val_dataset:\n",
    "    val_batches += 1\n",
    "print(f\"Số lượng batch trong val_dataset: {val_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đếm chính xác số mẫu trong train_dataset\n",
    "train_samples = 0\n",
    "for x_batch, _ in train_dataset:\n",
    "    train_samples += x_batch.shape[0]\n",
    "print(f\"Số lượng mẫu thực tế trong train_dataset: {train_samples}\")\n",
    "\n",
    "# Đếm chính xác số mẫu trong val_dataset\n",
    "val_samples = 0\n",
    "for x_batch, _ in val_dataset:\n",
    "    val_samples += x_batch.shape[0]\n",
    "print(f\"Số lượng mẫu thực tế trong val_dataset: {val_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_all(y_true, y_pred):\n",
    "    y_test_flat = y_true.reshape(-1, y_true.shape[-1]) \n",
    "    y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "    \n",
    "    r2 = r2_score(y_test_flat, y_pred_flat)\n",
    "    mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_flat, y_pred_flat))\n",
    "    mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "    \n",
    "    print(f\"R²: {r2 * 100:.4f}%\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "evaluate_all(y, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_all_by_target_features(y_true, y_pred):\n",
    "    y_true_flat = y_true.reshape(-1, y_true.shape[-1]) \n",
    "    y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "\n",
    "    for i in range(y_true_flat.shape[1]):\n",
    "        y_true_feature = y_true_flat[:, i]\n",
    "        y_pred_feature = y_pred_flat[:, i]\n",
    "        \n",
    "        r2 = r2_score(y_true_feature, y_pred_feature)\n",
    "        mse = mean_squared_error(y_true_feature, y_pred_feature)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true_feature, y_pred_feature)\n",
    "        \n",
    "        print(f\"Feature {i + 1}:\")\n",
    "        print(f\"  R²: {r2 * 100:.4f}%\")\n",
    "        print(f\"  MSE: {mse:.4f}\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE: {mae:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "evaluate_all_by_target_features(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-11T08:31:17.983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "target_features=['AT mean', 'AT max']\n",
    "station = 'Ca Mau'\n",
    "y_true_flat = y.reshape(-1, y.shape[-1])\n",
    "y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "    \n",
    "# Khởi tạo mảng để lưu dữ liệu đã inverse transform\n",
    "y_true_original = np.copy(y_true_flat)\n",
    "y_pred_original = np.copy(y_pred_flat)\n",
    "    \n",
    "# Scale ngược dữ liệu nếu scalers được cung cấp\n",
    "if scalers is not None and station in scalers:\n",
    "    station_scalers = scalers[station]\n",
    "        \n",
    "    for i, feature in enumerate(target_features):\n",
    "        if feature in station_scalers:\n",
    "            feature_scaler = station_scalers[feature]\n",
    "                \n",
    "            # Inverse transform từng cột riêng biệt\n",
    "            y_true_feature = y_true_flat[:, i].reshape(-1, 1)\n",
    "            y_pred_feature = y_pred_flat[:, i].reshape(-1, 1)\n",
    "                \n",
    "            y_true_original[:, i:i+1] = feature_scaler.inverse_transform(y_true_feature)\n",
    "            y_pred_original[:, i:i+1] = feature_scaler.inverse_transform(y_pred_feature)\n",
    "\n",
    "for i, feature in enumerate(target_features):\n",
    "    y_true_feature = y_true_original[:, i]\n",
    "    y_pred_feature = y_pred_original[:, i]\n",
    "        \n",
    "    r2 = r2_score(y_true_feature, y_pred_feature)\n",
    "    mse = mean_squared_error(y_true_feature, y_pred_feature)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_feature, y_pred_feature)\n",
    "        \n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Hệ số xác định (R²): {r2 * 100:.3f}%\")\n",
    "    print(f\"  Sai số bình phương trung bình (MSE): {mse:.3f}\")\n",
    "    print(f\"  Căn sai số bình phương trung bình (RMSE): {rmse:.3f}\")\n",
    "    print(f\"  Sai số tuyệt đối trung bình (MAE): {mae:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7827318,
     "sourceId": 12411199,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
